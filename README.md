# The WorldView Project

## Table of Contents
- [Introduction](#introduction)
- [Features](#features)
- [Setup and Installation](#setup-and-installation)
- [Usage](#usage)
  - [Surface-Level Analysis (-S)](#surface-level-analysis--s)
  - [Deep-Level Analysis (-D)](#deep-level-analysis--d)
  - [Text Parsing and Mining](#text-parsing-and-mining)
- [Equations and Computational Framework](#equations-and-computational-framework)
  - [Cosine Similarity](#cosine-similarity)
  - [Kruskal-Wallis Test](#kruskal-wallis-test)
  - [Latent Dirichlet Allocation (LDA)](#latent-dirichlet-allocation-lda)
- [License](#license)

---

## Introduction

The **WorldView Project** provides a framework to analyze responses generated by Large Language Models (LLMs), uncovering biases, reasoning structures, and thematic diversity.

---

## Features
- **Surface-Level Analysis (-S)**: High-level scoring for bias, ethical framing, and factual accuracy.
- **Deep-Level Analysis (-D)**: Advanced semantic similarity and thematic modeling.
- **Statistical Evaluation**: Apply Kruskal-Wallis, MANOVA, and Chi-square tests.
- **Visualization**: Generate graphs for interconnections and themes.

---

## Setup and Installation
1. Clone the repository
2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3. Ensure Python 3.8 or later is installed.

---

## Usage

### Surface-Level Analysis (-S)
The Surface-Level Analysis script focuses on scoring LLM responses based on:
- **Factual Accuracy**: A basic heuristic to score factuality.
- **Bias and Ethics**: Uses zero-shot classification to evaluate ethical framing and potential biases.
- **Interconnectedness**: Examines cross-topic cohesion using semantic embeddings.

#### Key Functions in WorldView-S.py:
1. **`score_factual_accuracy(response)`**:
   ```python
   def score_factual_accuracy(response):
       return len(response.split()) % 5  # Placeholder logic for scoring factuality
   ```
   - Placeholder logic that calculates factuality based on word count. Replaceable with a more advanced fact-checking method.

2. **`score_bias_type(response)`**:
   ```python
   def score_bias_type(response):
       classification = zero-shot_pipeline(response, candidate_labels=bias_labels, multi_label=True)
       return max(classification['scores']) if classification['scores'] else 0
   ```
   - Evaluates bias using zero-shot classification against a list of predefined labels like "Western-centric" or "economically biased."

3. **Statistical Tests**:
   Performs Kruskal-Wallis and Chi-square tests to identify significant differences between LLMs:
   ```python
   def perform_statistical_tests(data):
       results = {}
       for col in ["Factual_Accuracy", "Bias_Type"]:
           grouped_data = [group[col].values for _, group in data.groupby("LLM_ID")]
           h_stat, p_val = kruskal(*grouped_data)
           results[f"{col}_Kruskal_H"] = h_stat
           results[f"{col}_Kruskal_p"] = p_val
       return results
   ```

---

### Deep-Level Analysis (-D)
The Deep-Level Analysis script performs more nuanced computations to evaluate:
- **Semantic Similarity**: Measures alignment of LLM responses with reference prompts.
- **Thematic Diversity**: Identifies topics and themes using Latent Dirichlet Allocation (LDA).
- **Inter-topic Connectivity**: Constructs a network graph to analyze how responses are interconnected.

#### Key Functions in WorldView-D.py:
1. **`semantic_similarity(response, reference)`**:
   ```python
   def semantic_similarity(response, reference):
       response_embedding = embedding_model.encode(response, convert_to_tensor=True)
       reference_embedding = embedding_model.encode(reference, convert_to_tensor=True)
       return float(util.pytorch_cos_sim(response_embedding, reference_embedding).item())
   ```
   - Computes cosine similarity between the response and a reference prompt using semantic embeddings.

2. **`extract_themes(responses)`**:
   ```python
   def extract_themes(responses):
       vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
       dtm = vectorizer.fit_transform(responses)
       lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
       lda_model.fit(dtm)
       return lda_model.transform(dtm), vectorizer.get_feature_names_out()
   ```
   - Identifies latent topics in responses and their probabilities using LDA.

3. **`calculate_interconnections(responses)`**:
   ```python
   def calculate_interconnections(responses):
       embeddings = embedding_model.encode(responses, convert_to_tensor=True)
       similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings).numpy()
       graph = nx.Graph()
       for i in range(len(responses)):
           for j in range(i + 1, len(responses)):
               if similarity_matrix[i, j] > 0.2:
                   graph.add_edge(i, j, weight=similarity_matrix[i, j])
       return graph
   ```
   - Builds a similarity graph where nodes represent responses and edges represent significant semantic similarity.

#### Visualization:
- **Network Graph**: Illustrates inter-topic relationships.
  ```python
  def visualize_interconnections(graph, output_path="interconnections_graph.png"):
      plt.figure(figsize=(10, 8))
      nx.draw(graph, with_labels=True, node_color="lightblue", edge_color="gray", node_size=500, font_size=10)
      plt.savefig(output_path)
  ```

---

### Text Parsing and Mining
The scripts parse, extract, and mine textual responses from JSON files containing LLM outputs. This is a crucial preprocessing step before performing any analysis.

#### Key Parsing Functions:
1. **`extract_responses_json(file_paths)`** (used in both scripts):
   ```python
   import json

   def extract_responses_json(file_paths):
       responses = []
       for file_path in file_paths:
           with open(file_path, 'r', encoding='utf-8') as file:
               data = json.load(file)
               responses.extend(data.get("responses", []))
       return responses
   ```
   - **Inputs**: List of file paths to JSON files.
   - **Process**:
     - Opens each file and reads its content.
     - Extracts the `"responses"` field (expected to be a list of response objects).
   - **Outputs**: A list of response dictionaries.

2. **Response Preprocessing**:
   - The parsed responses are further structured into a DataFrame for analysis:
     ```python
     import pandas as pd

     def process_responses(responses):
         data = []
         for entry in responses:
             data.append({
                 "LLM_ID": entry["LLM_ID"],
                 "Set_ID": entry["Set_ID"],
                 "Response_Text": entry["Response"]
             })
         return pd.DataFrame(data)
     ```
   - Converts the response JSON into a tabular format for easier manipulation and analysis.

3. **Application in Analysis**:
   - After parsing and structuring, these responses are fed into scoring functions (e.g., semantic similarity or thematic modeling).

---

## Equations and Computational Framework

### Cosine Similarity
```python
from sentence_transformers import SentenceTransformer, util
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

def semantic_similarity(response, reference):
    response_embedding = embedding_model.encode(response, convert_to_tensor=True)
    reference_embedding = embedding_model.encode(reference, convert_to_tensor=True)
    return float(util.pytorch_cos_sim(response_embedding, reference_embedding).item())
```
- Measures how aligned two vector embeddings are, ranging from -1 (opposite) to +1 (aligned).

### Kruskal-Wallis Test
```python
from scipy.stats import kruskal

def perform_statistical_tests(data):
    results = {}
    for col in ["Semantic_Similarity", "Bias_Type"]:
        grouped_data = [group[col].values for _, group in data.groupby("LLM_ID")]
        h_stat, p_val = kruskal(*grouped_data)
        results[f"{col}_Kruskal_H"] = h_stat
        results[f"{col}_Kruskal_p"] = p_val
    return results
```
- Tests for statistical differences across multiple groups.

### Latent Dirichlet Allocation (LDA)
```python
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

def extract_themes(responses):
    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
    dtm = vectorizer.fit_transform(responses)
    lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
    lda_model.fit(dtm)
    return lda_model.transform(dtm), vectorizer.get_feature_names_out()
```
- Models thematic distributions to identify key topics and their prevalence.

---

## License
To use this project for research, email `ipcontrol@knowdyn.co.uk` with the subject: `Requesting #WorldView License`.

---

